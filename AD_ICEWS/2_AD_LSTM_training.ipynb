{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection using LSTM Encoder-Decoder.\n",
    "\n",
    "This is the second of four notebooks used in defining and training an encoder-decoder LSTM architecture for anomaly detection. Each of the following four steps are completed in an individual notebook:\n",
    "\n",
    "- __Preprocess__: Preprocess raw ICEWS data into time series for training and evaluating a model.\n",
    "- __Train__: Create and train a model with pre-processed and cleaned data.\n",
    "- __Threshold calculation__: Use the residuals from a validation set to determine an anomaly detection threshold.\n",
    "- __Inference__: Run anomaly detection on data from various countries to assess performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import sklearn.preprocessing as pre\n",
    "\n",
    "import joblib\n",
    "from util import data, preprocess, icews\n",
    "from models import networks\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data:\n",
    "train_data = joblib.load('data/icews_ml/7day_training.joblib')\n",
    "valid_data = joblib.load('data/icews_ml/7day_validate.joblib')\n",
    "\n",
    "# create test/train DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_data, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model\n",
    "- The variable ``window`` defines the length of the subsequences to be reconstructed by the LSTM autoencoder. The timeseries will be chunked according to this variable.\n",
    "\n",
    "- ``n_features`` is the number of features in the unencoded data and is used in conjunction with ``window`` size and a scaling factor to choose the embedding space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Anomaly Detection Period (days)\n",
    "window = 7 \n",
    "\n",
    "# Get number of variables, and embedding size\n",
    "n_features = 5 # Intensity, QC1, ..., QC4\n",
    "factor = 4\n",
    "emb_size = int(n_features * window * factor)\n",
    "\n",
    "# Initialize new model\n",
    "model = networks.LSTMEncoderDecoder(n_features, emb_size)\n",
    "\n",
    "print(\"embedding space size: \", emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "The model is then trained using a pre-defined autoencoder training function that can be found in ``models/networks.py``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = nn.MSELoss(reduction='sum')\n",
    "loss = networks.train_encoder(model, 500, \n",
    "                     train_loader, \n",
    "                     criterion=objective,\n",
    "                     lr=1e-4,\n",
    "                     testload=valid_loader, \n",
    "                     reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=loss)\n",
    "plt.legend(['Training Loss', 'Valid Loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'data/models/ae_lstm_mse_sum_{window}d_{factor}f.pt')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python [conda env:general-ml] *",
   "language": "python",
   "name": "conda-env-general-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
